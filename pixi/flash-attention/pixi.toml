[workspace]
authors = ["Guilherme Leobas <guilhermeleobas@gmail.com>"]
channels = ["conda-forge"]
name = "flash-attention"
platforms = ["osx-arm64"]
version = "0.1.0"

[activation.env]
FLASH_ATTENTION_DISABLE_BACKWARD="TRUE"
FLASH_ATTENTION_DISABLE_SPLIT="TRUE"
FLASH_ATTENTION_DISABLE_SOFTCAP="TRUE"
FLASH_ATTENTION_DISABLE_LOCAL="TRUE"
FLASH_ATTENTION_DISABLE_CLUSTER="TRUE"
FLASH_ATTENTION_DISABLE_VARLEN="TRUE"
FLASH_ATTENTION_DISABLE_PACKGQA="TRUE"
FLASH_ATTENTION_DISABLE_PAGEDKV="TRUE"
FLASH_ATTENTION_DISABLE_APPENDKV="TRUE"
FLASH_ATTENTION_DISABLE_FP8="TRUE"
FLASH_ATTENTION_DISABLE_FP16="FALSE"
FLASH_ATTENTION_DISABLE_FP32="TRUE"
FLASH_ATTENTION_DISABLE_HDIM96="TRUE"
FLASH_ATTENTION_DISABLE_HDIM128="TRUE"
FLASH_ATTENTION_DISABLE_HDIM192="TRUE"
FLASH_ATTENTION_DISABLE_HDIM256="TRUE"

[tasks]

[dependencies]
packaging = ">=25.0,<26"
transformers = ">=4.57.0,<5"
accelerate = ">=1.10.1,<2"

[pypi-dependencies]
torch = "*"
packaging = "*"
transformers = "*"
accelerate = "*"

